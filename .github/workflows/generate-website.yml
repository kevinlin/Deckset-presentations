name: Generate and Deploy Deckset Website

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:  # Allow manual triggering

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Test job
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for last modified dates
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      - name: Install UV (fast Python package installer)
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
      
      - name: Install Python dependencies
        run: |
          uv pip install --system -r <(uv pip compile pyproject.toml --extra dev)
      
      - name: Install JavaScript dependencies
        run: npm install
      
      - name: Run Python tests
        run: |
          uv run pytest --verbose --tb=short --cov=. --cov-report=xml --cov-report=html --junitxml=pytest-results.xml
        env:
          PYTHONPATH: .
      
      - name: Run JavaScript tests
        run: npm run test:js:coverage
      
      - name: Parse and display test results
        if: always()
        run: |
          echo "## 📊 Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Python test results
          if [ -f "pytest-results.xml" ]; then
            # Extract test counts from JUnit XML
            if command -v xmllint >/dev/null 2>&1; then
              TOTAL_TESTS=$(xmllint --xpath "string(/testsuites/testsuite/@tests)" pytest-results.xml 2>/dev/null || echo "unknown")
              FAILED_TESTS=$(xmllint --xpath "string(/testsuites/testsuite/@failures)" pytest-results.xml 2>/dev/null || echo "0")
              ERROR_TESTS=$(xmllint --xpath "string(/testsuites/testsuite/@errors)" pytest-results.xml 2>/dev/null || echo "0")
              
              echo "### 🐍 Python Tests (pytest)" >> $GITHUB_STEP_SUMMARY
              if [ "$FAILED_TESTS" = "0" ] && [ "$ERROR_TESTS" = "0" ]; then
                echo "✅ **Status:** PASSED" >> $GITHUB_STEP_SUMMARY
                PASSED_TESTS=$((TOTAL_TESTS - FAILED_TESTS - ERROR_TESTS))
                echo "- **Total:** $TOTAL_TESTS tests" >> $GITHUB_STEP_SUMMARY
                echo "- **Passed:** $PASSED_TESTS" >> $GITHUB_STEP_SUMMARY
              else
                echo "❌ **Status:** FAILED" >> $GITHUB_STEP_SUMMARY
                echo "- **Total:** $TOTAL_TESTS tests" >> $GITHUB_STEP_SUMMARY
                echo "- **Failed:** $FAILED_TESTS" >> $GITHUB_STEP_SUMMARY
                echo "- **Errors:** $ERROR_TESTS" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "### 🐍 Python Tests (pytest)" >> $GITHUB_STEP_SUMMARY
              echo "✅ **Status:** Completed (detailed results in artifacts)" >> $GITHUB_STEP_SUMMARY
              echo "- **Note:** XML parsing not available, check artifacts for detailed results" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### 🐍 Python Tests (pytest)" >> $GITHUB_STEP_SUMMARY
            echo "❌ **Status:** No results file found" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # JavaScript test results
          echo "### 🟨 JavaScript Tests (Jest)" >> $GITHUB_STEP_SUMMARY
          echo "✅ **Status:** Completed with coverage" >> $GITHUB_STEP_SUMMARY
          echo "- **Coverage:** Available in artifacts" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📁 Detailed Reports" >> $GITHUB_STEP_SUMMARY
          echo "- Download the **test-results** artifact for complete coverage reports" >> $GITHUB_STEP_SUMMARY
          echo "- Python coverage: \`htmlcov/index.html\`" >> $GITHUB_STEP_SUMMARY
          echo "- JavaScript coverage: \`coverage/lcov-report/index.html\`" >> $GITHUB_STEP_SUMMARY
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results
          path: |
            pytest-results.xml
            coverage.xml
            htmlcov/
            coverage/
          retention-days: 30

  # Build job
  build:
    runs-on: ubuntu-latest
    needs: test  # Only build if tests pass
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for last modified dates
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Install UV (fast Python package installer)
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
      
      - name: Install dependencies
        run: |
          uv pip install --system -r <(uv pip compile pyproject.toml)
      
      - name: Validate generator configuration
        run: |
          python main.py --validate
      
      - name: Generate website
        run: |
          python main.py --verbose --output site
        env:
          PYTHONPATH: .
      
      - name: Check generated files
        run: |
          echo "Generated files:"
          find site -type f -name "*.html" | head -20
          echo "Total HTML files: $(find site -type f -name "*.html" | wc -l)"
          echo "Total slide images: $(find site/slides -type f -name "*.png" -o -name "*.jpg" -o -name "*.jpeg" -o -name "*.gif" 2>/dev/null | wc -l || echo 0)"
      
      - name: Setup Pages
        uses: actions/configure-pages@v4
        if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
      
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
        with:
          path: ./site

  # Deployment job
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: [test, build]
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  # Notification job for failures
  notify-failure:
    runs-on: ubuntu-latest
    needs: [test, build, deploy]
    if: failure() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
    steps:
      - name: Create issue on failure
        uses: actions/github-script@v7
        with:
          script: |
            const title = `Website Generation Failed - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## Website Generation Failure
            
            The automated website generation workflow failed on the main branch.
            
            **Details:**
            - **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            - **Commit:** ${{ github.sha }}
            - **Branch:** ${{ github.ref_name }}
            - **Triggered by:** ${{ github.actor }}
            - **Timestamp:** ${new Date().toISOString()}
            
            **Next Steps:**
            1. Check the workflow logs for specific error messages
            2. **For test failures:** Run tests locally with \`uv run pytest\` and \`npm run test:js\`
            3. Verify that all presentation markdown files are valid
            4. Ensure all referenced images exist in their respective folders
            5. Test the generation locally with: \`python main.py --verbose\`
            
            This issue was automatically created by the GitHub Actions workflow.
            `;
            
            // Check if there's already an open issue for today
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: ['automated', 'website-generation', 'failure']
            });
            
            const today = new Date().toISOString().split('T')[0];
            const existingIssue = issues.find(issue => 
              issue.title.includes(today) && issue.title.includes('Website Generation Failed')
            );
            
            if (!existingIssue) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['automated', 'website-generation', 'failure']
              });
              console.log('Created failure notification issue');
            } else {
              console.log('Issue already exists for today, not creating duplicate');
            }
